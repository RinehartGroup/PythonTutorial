{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented Programming and Documentation\n",
    "\n",
    "There are many good resources which provide an introduction to object oriented programming. This tutorial won't provide a comprehensive introduction and will assume some prior knowledge. [Here's a good starting point.](https://realpython.com/python3-object-oriented-programming/)\n",
    "\n",
    "We are interested in using object oriented programming because it lets use represent things that we are familiar with in code. For example, concrete things like data files can be represented as classes. Classes let us group together data and functions that operate on the data; when we are working with the data in a notebook, we can then create as many objects of a class as there are data files, for example. We can also represent more abstract things, like datasets or models, as classes.\n",
    "\n",
    "Note that [there are several different programming paradigms](https://www.geeksforgeeks.org/introduction-of-programming-paradigms/). We'll likely stick to object oriented and functional programming, but there aren't hard rules for when to use one or the other, and within a single project you may find yourself using both.\n",
    "\n",
    "### An update on the bug\n",
    "\n",
    "The bug discussed in the previous tutorial arises again here, this time affecting our ability to read the header (technically the same issue existed in the last tutorial but we weren't using any info within the header). For some reason, .dat files are sometimes comma separated and sometimes space separated. I think Excel is to blame (that is, if you open a .dat file in Excel and save it, it will be comma separated). We'll need to update our code to handle both cases.\n",
    "\n",
    "The functions for reading both headers and data sections now account for this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes we'll write in this tutorial will be able to handle all of the following files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvsh1_path = Path(\"data/mvsh1.dat\")\n",
    "mvsh2_path = Path(\"data/mvsh2.dat\")\n",
    "mvsh2a_path = Path(\"data/mvsh2a.dat\")\n",
    "mvsh2b_path = Path(\"data/mvsh2b.dat\")\n",
    "mvsh3_path = Path(\"data/mvsh3.dat\")\n",
    "mvsh4_path = Path(\"data/mvsh4.dat\")\n",
    "zfcfc1_path = Path(\"data/zfcfc1.dat\")\n",
    "zfcfc2_path = Path(\"data/zfcfc2.dat\")\n",
    "zfcfc3_path = Path(\"data/zfcfc3.dat\")\n",
    "fc4a_path = Path(\"data/fc4a.dat\")\n",
    "fc4b_path = Path(\"data/fc4b.dat\")\n",
    "zfc4a_path = Path(\"data/zfc4a.dat\")\n",
    "zfc4b_path = Path(\"data/zfc4b.dat\")\n",
    "zfcfc4_path = Path(\"data/zfcfc4.dat\")\n",
    "dataset4_path = Path(\"data/dataset4.dat\")\n",
    "\n",
    "all_files = [\n",
    "    mvsh1_path, mvsh2_path, mvsh2a_path, mvsh2b_path, mvsh3_path, mvsh4_path,\n",
    "    zfcfc1_path, zfcfc2_path, zfcfc3_path, fc4a_path, fc4b_path, zfc4a_path,\n",
    "    zfc4b_path, zfcfc4_path, dataset4_path\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions `label_clusters()`, `unique_values()`, `find_outlier_indices()`, and `find_temp_turnaround_point()` from the previous tutorial have been rewritten here. Some have been slightly modified to handle the data we'll find in this tutorial.\n",
    "\n",
    "Examples of code documentation in the form of doc strings have been added to these functions. Docstrings are a way of documenting functions, classes, and modules in Python. They are enclosed in triple quotes and are the first thing in a function, class, or module. They are accessible via the `__doc__` attribute of the function, class, or module. For example, `label_clusters.__doc__` will return the doc string for the `label_clusters()` function.\n",
    "\n",
    "[Here is an excellent video from ArjanCodes on code documentation](https://youtu.be/L7Ry-Fiij-M). The whole video is worth watching, but section 3 (starting at 10:44) is specifically about docstrings. As was done in the video, the templates for the docstrings are automatically created in VS Code using the \"autoDocstring - Python Docstring Generator\" extension, which you should install for use later in this tutorial. The format we'll be using is the [numpy docstring format](https://numpydoc.readthedocs.io/en/latest/format.html).\n",
    "\n",
    "Docstrings and proper Python type hinting add a lot of value to code, especially when working in an IDE which fully utilizes them. Take note of how VS Code pop ups help you when working with the functions in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters(\n",
    "        vals: pd.Series,\n",
    "        eps: float = 0.001,\n",
    "        min_samples: int = 10\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"For determining the nominal values of data in a series containing one or more\n",
    "    nominal values with some noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vals : pd.Series\n",
    "        A series of data containing one or more nominal values with some noise.\n",
    "    eps : float, optional\n",
    "        Passed to `sklearn.cluster.DBSCAN()`. The maximum distance between two samples\n",
    "        for one to be considered as in the neighborhood of the other, by default 0.001.\n",
    "    min_samples : int, optional\n",
    "        Passed to `sklearn.cluster.DBSCAN()`. The number of samples in a neighborhood\n",
    "        for a point to be considered as a core point, by default 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of the same size as `vals` which contains the cluster labels for each\n",
    "        element in `vals`. Noisy samples are given the label -1. A `vals` series\n",
    "        containing, for example, one nominal temperature with noise should return an\n",
    "        array with only one cluster label of -1.\n",
    "   \n",
    "    \"\"\"\n",
    "    reshaped_vals = vals.values.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    reshaped_normalized_vals = scaler.fit_transform(reshaped_vals)\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    cluster_labels = dbscan.fit_predict(reshaped_normalized_vals)\n",
    "    return cluster_labels\n",
    "\n",
    "def unique_values(\n",
    "    x: pd.Series, eps: float = 0.001, min_samples: int = 10\n",
    ") -> list[float]:\n",
    "    \"\"\"Given a series of data containing one or more nominal values with some noise,\n",
    "    returns a list of the nominal values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pd.Series\n",
    "        A series of data containing one or more nominal values with some noise.\n",
    "    eps : float, optional\n",
    "        Passed to `sklearn.cluster.DBSCAN()`. The maximum distance between two samples\n",
    "        for one to be considered as in the neighborhood of the other, by default 0.001.\n",
    "    min_samples : int, optional\n",
    "        Passed to `sklearn.cluster.DBSCAN()`. The number of samples in a neighborhood\n",
    "        for a point to be considered as a core point, by default 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[float]\n",
    "        The nominal values in `x` with the noise removed.\n",
    "    \"\"\"\n",
    "    cluster_labels = label_clusters(x, eps=eps, min_samples=min_samples)\n",
    "    unique_values = []\n",
    "    for i in np.unique(cluster_labels):\n",
    "        # average the values in each cluster\n",
    "        unique_val = np.mean(x[cluster_labels == i])\n",
    "        unique_val = round(unique_val, 1)\n",
    "        unique_values.append(unique_val)\n",
    "    return unique_values\n",
    "\n",
    "def find_outlier_indices(x: pd.Series, threshold: float = 3) -> list[int]:\n",
    "    \"\"\"Finds the indices of outliers in a series of data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pd.Series\n",
    "        A series of data.\n",
    "    threshold : float, optional\n",
    "        The number of standard deviations from the mean to consider a value an outlier,\n",
    "        by default 3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[int]\n",
    "        The indices of the outliers in `x`.\n",
    "    \"\"\"\n",
    "    z_scores = (x - x.mean()) / x.std()\n",
    "    outliers = z_scores.abs() > threshold\n",
    "    return list(outliers[outliers].index)\n",
    "\n",
    "def find_temp_turnaround_point(df: pd.DataFrame) -> int:\n",
    "    \"\"\"Finds the index of the temperature turnaround point in a dataframe of\n",
    "    a ZFCFC experiment which includes a column \"Temperature (K)\". Can handle two cases\n",
    "    in which a single dataframe contains first a ZFC experiment, then a FC experiment.\n",
    "    Case 1: ZFC temperature monotonically increases, then FC temperature monotonically\n",
    "    decreases. Case 2: ZFC temperature monotonically increases, the temperature is\n",
    "    reset to a lower value, then FC temperature monotonically increases. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A dataframe of a ZFCFC experiment which includes a column \"Temperature (K)\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the temperature turnaround point.\n",
    "            \n",
    "    \"\"\"\n",
    "    outlier_indices = find_outlier_indices(df['Temperature (K)'].diff())\n",
    "    if len(outlier_indices) == 0:\n",
    "        # zfc temp increases, fc temp decreases\n",
    "        zero_point = abs(df['Temperature (K)'].iloc[20:-20].diff()).idxmin()\n",
    "        return zero_point\n",
    "    else:\n",
    "        # zfc temp increases, reset temp, fc temp increases\n",
    "        return outlier_indices[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a few classes to organize our datafiles into a useful form for further use in data analysis. The first will be one to handle individual .dat files, then a second to handle a dataset containing several .dat files on the same sample, and finally a third that will organize sample information.\n",
    "\n",
    "Standard Python classes typically start with an `__init__()` function, which is called when an object of the class is created. Class attributes (variables that are attached to the classes) are typically defined here. The `self` argument is required for all class functions and attributes. It is a reference to the object itself, and is used to access attributes and functions of the class. The [linked Real Python article](https://realpython.com/python3-object-oriented-programming/) gives a good explanation of `self` and how it is used.\n",
    "\n",
    "An outline of the `DatFile` class has been given in the following cell. The `__init__()` function contains attribute definitions, where the values of the attributes are determined by various functions within the class. This is a good pattern for keeping the `__init__()` function readable -- instead of writing out the code to determine the attribute values, we can just call the functions that do that work. The functions will be defined later in the class.\n",
    "\n",
    "Several functions have already been written. Most of these are adapted from previous tutorials. Take note of the differences when used in classes -- for example, `_get_comments()` no longer needs a `file` argument, since the `self` argument gives it access to the `local_path` attribute found withing the `DatFile` object.\n",
    "\n",
    "A note on style: the functions `_get_comments()`, `_get_header()`, and `_get_data()` are prefixed with an underscore. This is a convention that indicates that the function is intended to be used only within the class. It is not enforced by Python, but is a good practice to follow.\n",
    "\n",
    "Note that the function definitions for `_determine_length()`, `_determine_hash()`, and `_get_date_created()` are single lines with the word `pass`. The `pass` keyword is used as a placeholder for code that will be written later. In this case, it's useful because we generally know what we want to do within the `__init__()` class but we need the class to have partial functionality while we write the rest of it. For example, you'll need access to the `local_path` and `header` attributes when writing those functions, and it's helpful to have a functioning partially defined class to work with while writing the rest of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatFile:\n",
    "    def __init__(self, file_path: str | Path) -> None:\n",
    "        self.local_path: Path = Path(file_path)\n",
    "        self.header: list[list[str]] = self._read_header()\n",
    "        self.data: pd.DataFrame = self._read_data()\n",
    "        self.comments: OrderedDict[str, list[str]] = self._get_comments()\n",
    "        self.length: int = self._determine_length()\n",
    "        self.sha512: str = self._determine_hash()\n",
    "        self.date_created: datetime = self._get_date_created()\n",
    "        self.experiments_in_file: list[str] = self._get_experiments_in_file()\n",
    "\n",
    "    # Exercise 3.1 code here\n",
    "\n",
    "    def _read_header(self, delimiter: str = \"\\t\") -> list[list[str]]:\n",
    "        header: list[list[str]] = []\n",
    "        with self.local_path.open() as f:\n",
    "            reader = csv.reader(f, delimiter=delimiter)\n",
    "            for row in reader:\n",
    "                header.append(row)\n",
    "                if row[0] == \"[Data]\":\n",
    "                    break\n",
    "        if len(header[2]) == 1:\n",
    "            # some .dat files have a header that is delimited by commas\n",
    "            header = self._read_header(delimiter=\",\")\n",
    "        return header\n",
    "\n",
    "    def _read_data(self, sep: str = \"\\t\",) -> pd.DataFrame:\n",
    "        skip_rows = len(self.header)\n",
    "        df = pd.read_csv(self.local_path, sep=sep, skiprows=skip_rows)\n",
    "        if df.shape[1] == 1:\n",
    "            # some .dat files have a header that is delimited by commas\n",
    "            df = self._read_data(sep=\",\")\n",
    "        return df\n",
    "    \n",
    "    def _get_comments(self) -> OrderedDict[str, list[str]]:\n",
    "        comments = self.data['Comment'].dropna()\n",
    "        comments = OrderedDict(comments)\n",
    "        for key, value in comments.items():\n",
    "            comments[key] = [comment.strip() for comment in value.split(',')]\n",
    "        return comments\n",
    "        \n",
    "    def _determine_length(self) -> int:\n",
    "        # Exercise 3.2 code here\n",
    "        pass\n",
    "\n",
    "    def _determine_hash(self) -> str:\n",
    "        # Exercise 3.3 code here\n",
    "        pass\n",
    "\n",
    "    def _get_date_created(self) -> str:\n",
    "        # Exercise 3.4 code here\n",
    "        pass\n",
    "    \n",
    "    def _get_experiments_in_file(self) -> list[str]:\n",
    "        experiments = []\n",
    "        if self.comments:\n",
    "            for comments in self.comments.values():\n",
    "                for comment in comments:\n",
    "                    if comment.lower() in [\"mvsh\", \"zfc\", \"fc\", \"zfcfc\"]:\n",
    "                        experiments.append(comment.lower())\n",
    "        else:\n",
    "            if len(self.data['Magnetic Field (Oe)'].unique()) == 1:\n",
    "                experiments.append('zfcfc')\n",
    "            else:\n",
    "                experiments.append('mvsh')\n",
    "        return experiments\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "Go back to the definition of the `DatFile` class and write the `__str__()` and `__repr__()` functions for the `DatFile` class. In this case, both should return a string with the format: \"DatFile(<name of file>)\". For example, if the file name is \"mvsh1.dat\" then `__str__()` and `__repr__()` should both return \"DatFile(mvsh1.dat)\".\n",
    "\n",
    "Remember that by using the `self` argument you have access to all attributes within the class, including the `local_path` attribute, which contains the file as a `Path` object. One of the `Path` methods should be useful here.\n",
    "\n",
    "The `__str__()` function is what is called when you `print()` an instance of the class. The `__repr__()` is the \"official\" string representation of the object, and is what is returned when you call `repr()` on an instance of the class. [Here is a good explanation of the difference between the two](https://stackoverflow.com/questions/1436703/difference-between-str-and-repr). In this case they can be the same thing, but that isn't always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert str(DatFile(mvsh1_path)) == \"DatFile(mvsh1.dat)\"\n",
    "assert repr(DatFile(dataset4_path)) == \"DatFile(dataset4.dat)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "Go back to the definition of the `DatFile` class and finish writing the `_determine_length()` function. `_determine_length()` should return the size in bytes of the file. See [`Path.stat()`](https://docs.python.org/3/library/stat.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert DatFile(mvsh2_path).length == 277199\n",
    "assert DatFile(zfcfc2_path).length == 255376"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "Go back to the definition of the `DatFile` class and finish writing the `_determine_hash()` function. `_determine_hash()` should return the SHA512 hash of the file. [See this Stack Overflow post](https://stackoverflow.com/questions/22058048/hashing-a-file-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert DatFile(mvsh3_path).sha512 == \"ea925d9931781ce2797c5ced4825d09f2a1254e6ee0ec453667b896ec5d7eaa366680c32138c14ed42a4fa9df9d719d32e052a32c2f2201ce6eff7ac63909c94\"\n",
    "assert DatFile(zfcfc3_path).sha512 == \"2771939279adecf506904d637cc0eb312c97c68926ac90f9a59fc37dd11505a61e6a2e1fe403c8fa22fbcb09b143d3f65c8d9a1e012193c83373dd926e92ad99\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "Go back to the definition of the `DatFile` class and finish writing the `_get_date_created()` function. `_get_date_created()` should return the date the file was created as a `datetime` object. In this case, the .dat file header contains information about the date the file was created. Use that rather than, for example `Path.stat().st_ctime`, which returns the date the file was last changed. Here's a good [deep dive on datetime](https://youtu.be/TFa38ONq5PY), though you can probably figure out what you need for this function from the [Python docs](https://docs.python.org/3/library/datetime.html).\n",
    "\n",
    "Note that in the `assert` tests below we use one of the string representations of the datetime object, in particular the ISO formatted date. One of the nice things about the `datetime` class is the easy way in which you can read in dates of standard formats and output them to other standard formats. It's a good argument for writing data files that contain date information in a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert DatFile(mvsh1_path).date_created.isoformat() == \"2020-07-11T11:07:00\"\n",
    "assert DatFile(mvsh4_path).date_created.isoformat() == \"2022-05-03T22:44:00\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see the benefit of classes by how our workflow changes now that we have the `DatFile` class to work with. We can make as many objects of the `DatFile` class as we want, and each one will have all of the attributes and functions defined in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_files:\n",
    "    dat_file = DatFile(file)\n",
    "    print(f\"{dat_file} experiments: {dat_file.experiments_in_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvsh1 = DatFile(mvsh1_path)\n",
    "mvsh1.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfcfc4 = DatFile(zfcfc4_path)\n",
    "zfcfc4.comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Let's write a new class, `Dataset`, to handle datasets, which we'll define as the collection of files associated with a single sample.\n",
    "\n",
    "Two important concepts in object-oriented programming are [inheritance and composition](https://realpython.com/inheritance-composition-python/). Composition is perhaps easier to understand, and what we'll be using here. The `Dataset` class will be composed of `DatFile` objects -- that is, the `Dataset.files` attribute will be a list of `DatFile` objects. The `Dataset` class will have its own attributes and functions, but it will also have access to all of the attributes and functions of the `DatFile` class. Here's [a more advanced discussion of composition and inheritance](https://youtu.be/0mcP8ZpUR38), and why composition is generally preferred when possible to make code more flexible.\n",
    "\n",
    "We'll reserve the next cell for the `SampleInfo` class, which we'll come back to later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SampleInfo:\n",
    "    # Exercise 3.7 code here\n",
    "\n",
    "    # Exercise 3.8 code here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains an outline for the `Dataset` class with the `__init__()` and `_get_data_from_commented_file()` methods already written. More on the latter method in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, id: str, dat_files: list[DatFile]) -> None:\n",
    "        self.id: str = id\n",
    "        self.files: list[DatFile] = dat_files\n",
    "        self.mvsh: dict[float, pd.DataFrame] = self._get_mvsh()\n",
    "        self.zfc: dict[float, pd.DataFrame] = self._get_zfc()\n",
    "        self.fc: dict[float, pd.DataFrame] = self._get_fc()\n",
    "        self.sample_info: SampleInfo = self._get_sample_info()\n",
    "\n",
    "    # Exercise 3.5 code here\n",
    "\n",
    "    def _get_mvsh(self) -> dict[float, pd.DataFrame]:\n",
    "        # Exercise 3.6 code here\n",
    "        pass\n",
    "\n",
    "    def _get_zfc(self) -> dict[float, pd.DataFrame]:\n",
    "        # Exercise 3.6 code here\n",
    "        pass\n",
    "\n",
    "    def _get_fc(self) -> dict[float, pd.DataFrame]:\n",
    "        # Exercise 3.6 code here\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_data_from_commented_file(file: DatFile, experiment: str) -> dict[float, pd.DataFrame]:\n",
    "        data = file.data\n",
    "        comments = file.comments\n",
    "        experiment_dfs = {}\n",
    "        for i, (dat_idx, comment_list) in enumerate(comments.items()):\n",
    "            if experiment.lower() in map(str.lower, comment_list):\n",
    "                for comment in comment_list:\n",
    "                    if match := re.search('\\d+', comment):\n",
    "                        nominal_value = float(match.group())\n",
    "                        break\n",
    "                start_idx = dat_idx + 1\n",
    "                end_idx = list(comments.keys())[i+1] if i+1 < len(comments) else (len(data))\n",
    "                experiment_dfs[nominal_value] = data.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        return experiment_dfs\n",
    "    \n",
    "    def _get_sample_info(self) -> SampleInfo:\n",
    "        # Exercise 3.7 code here\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell should run at any stage of your progress on `Dataset`. It creates a `Dataset` object for each sample in the \"data\" folder.\n",
    "\n",
    "Similar to the last tutorial, the files in the \"data\" folder contain .dat files formatted in several different ways:\n",
    "- \"mvsh1.dat\" and \"mvsh2.dat\" contain multiple M vs H experiments at different nominal temperatures within the same files.\n",
    "- \"mvsh2a.dat\" and \"mvsh2b.dat\" contain the same experiments as \"mvsh2.dat\", but with separate files for the 5 and 300 K experiments. \"mvsh3.dat\" also contains a single M vs H experiment.\n",
    "- \"mvsh4.dat\" contains a single M vs H experiment with a user-created comment within the \"[Data]\" section of the file indicating the nominal temperature of the experiment.\n",
    "- \"zfcfc1.dat\", \"zfcfc2.dat\", and \"zfcfc3.dat\" contain ZFC and FC experiments at one nominal field in each file, though the first two have ZFC and FC experiments both with monotonically increasing temperatures while the third has a ZFC with monotonically increasing temperatures and an FC with monotonically decreasing temperatures.\n",
    "- \"zfc4a.dat\" and \"zfc4b.dat\" contain ZFC experiments at 100 and 1000 Oe respectively. \"fc4a.dat\" and \"fc4b.dat\" contain FC experiments at 100 and 1000 Oe respectively. All contain user-created comments within the \"[Data]\" section of the file indicating the nominal field of the experiment.\n",
    "- \"zfcfc4.dat\" contains the previously mentioned ZFC and FC experiments for sample 4 but all in one file. The experiments are separated by user-created comments.\n",
    "- \"dataset4.dat\" contains the data within \"zfcfc4.dat\" and \"mvsh4.dat\", with user-created comments separating the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell before running assert tests\n",
    "dset1 = Dataset(\"dset1\", [DatFile(mvsh1_path), DatFile(zfcfc1_path)])\n",
    "dset2_1 = Dataset(\"dset2\", [DatFile(mvsh2_path), DatFile(zfcfc2_path)])\n",
    "dset2_2 = Dataset(\"dset2\", [DatFile(mvsh2a_path), DatFile(mvsh2b_path), DatFile(zfcfc2_path)])\n",
    "dset3 = Dataset(\"dset3\", [DatFile(mvsh3_path), DatFile(zfcfc3_path)])\n",
    "dset4_1 = Dataset(\"dset4\", [DatFile(mvsh4_path), DatFile(zfcfc4_path)])\n",
    "dset4_2 = Dataset(\"dset4\", [DatFile(mvsh4_path), DatFile(zfc4a_path), DatFile(zfc4b_path), DatFile(fc4a_path), DatFile(fc4b_path)])\n",
    "dset4_3 = Dataset(\"dset4\", [DatFile(dataset4_path)])\n",
    "\n",
    "all_datasets = [dset1, dset2_1, dset2_2, dset3, dset4_1, dset4_2, dset4_3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "Write the `__str__()` and `__repr__()` functions for the `Dataset` class. In this case, both should return a string with the format: \"Dataset(<name of sample>)\", where the name of the sample is the `Dataset.id` attribute. For example, if the sample name is \"dset1\" then `__str__()` and `__repr__()` should both return \"Dataset(dset1)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert str(dset1) == repr(dset1) == \"Dataset(dset1)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Write the `_get_mvsh()`, `_get_zfc()`, and `_get_fc()` functions for the `Dataset` class. You should be able to use combinations of functions you wrote for the previous tutorial.\n",
    "\n",
    "- `_get_mvsh()` should return a dictionary with keys corresponding to the nominal temperatures of individual M vs H experiments with values of dataframes containing the data for each experiment. For example, if the sample has M vs H experiments at 5 and 300 K, then `_get_mvsh()` should return a dictionary with keys of 5 and 300 and values of dataframes containing the data for each experiment.\n",
    "- `_get_zfc()` and `_get_fc()` should return a dictionary with the keys corresponding to the nominal fields of individual ZFC or FC experiments with values of dataframes containing the data for each experiment. For example, if the sample has ZFC experiments at 100 and 1000 Oe, then `_get_zfc()` should return a dictionary with keys of 100 and 1000 and values of dataframes containing the data for each experiment.\n",
    "\n",
    "All three functions should have the same general structure:\n",
    "1. Go through the `Dataset.files` attribute and find the files that contain the relevant experiments. Note that for ZFC (FC) experiments the `file.experiments_in_file` may be either \"zfc\" (\"fc\") or \"ZFCFC\". [Python `set()` methods may help you here.](https://realpython.com/python-sets/)\n",
    "2. Create an empty dictionary\n",
    "3. Go through the files that contain the relevant experiments and add the data for each experiment to the dictionary. If the files contain user-commented data, you'll need to use `Dataset._get_data_from_commented_file()` to get the data. Otherwise you'll need to implement the algorithms we created in the previous tutorial to get the data. [See the `dict.update()` method.](https://www.geeksforgeeks.org/python-dictionary-update-method/)\n",
    "\n",
    "#### Static Methods\n",
    "\n",
    "You may have noticed a `@staticmethod` line above the definition of `_get_data_from_commented_file()`. The `@` symbol indicates that the following line is a [decorator](https://realpython.com/primer-on-python-decorators/). A [\"static method\"](https://realpython.com/instance-class-and-static-methods-demystified/) is a method that does not require an instance of the class to be called (i.e., it doesn't need the `self` argument). It is placed in the class because it is very closely tied to what is going on in the class. Technically, you can call `_get_data_from_commented_file()` outside of the class, but it's unlikely that you'll need to. If the function were potentially useful outside the classs, you would define it outside the class, as we did with `label_clusters()`, `unique_values()`, `find_outlier_indices()`, and `find_temp_turnaround_point()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling `_get_data_from_commented_file()` from outside of a class instance works\n",
    "some_data = Dataset._get_data_from_commented_file(DatFile(dataset4_path), 'mvsh')\n",
    "some_data[293].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(dset1.mvsh.keys()) == [2, 4, 6, 8, 10, 12, 300]\n",
    "assert dset1.mvsh[2].shape == (1124, 89)\n",
    "assert list(dset1.zfc.keys()) == [100]\n",
    "assert dset1.zfc[100].shape == (252, 89)\n",
    "assert dset2_1.mvsh.keys() == dset2_2.mvsh.keys()\n",
    "assert dset2_1.mvsh[5][\"Magnetic Field (Oe)\"].equals(dset2_2.mvsh[5][\"Magnetic Field (Oe)\"])\n",
    "assert dset4_1.mvsh.keys() == dset4_2.mvsh.keys() == dset4_3.mvsh.keys()\n",
    "assert dset4_1.zfc.keys() == dset4_2.zfc.keys() == dset4_3.zfc.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add one more bit of utility to the `Dataset` class. Since all files within a dataset pertain to a single magnetometry sample, it would be useful to be able to get sample information (e.g. mass, molecular weight, diamagnetic correction, etc.) from the dataset. That sample information is stored in the header of the .dat files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dset1.files[0].header:\n",
    "    if line[0] == \"INFO\":\n",
    "        print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information would be well-suited to a `dataclass`. Python `dataclass` objects are similar to classes, but are intended to primarily store data rather than perform operations. They have a convenient format for defining them and come with some useful built-in methods. Here's [a great intro video](https://youtu.be/CvQ7e6yUtnw) and [article](https://realpython.com/python-data-classes/).\n",
    "\n",
    "In short, we can define a `dataclass` using the `@dataclass` decorator (making sure to include a `from dataclasses import dataclass` at the top of the file). Defining the attributes of the class simply requires defining the name and type of the attribute. We then get `__init__()`, `__str__()`, and `__repr__()` methods for free. We can also define methods within the class as we would for a normal class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleSampleInfo:\n",
    "    material: str | None = None\n",
    "    mass: float | None = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class uses optional arguments -- the type of `material` is a `str` or `None` (where the pipe symbol, `|`, means or), with a default of `None`. We need the arguments of `SampleData` to be optional because most of the time the .dat files won't have all of the fields filled in.\n",
    "\n",
    "Let's make a couple instances of the `SimpleSampleInfo` class to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple1 = SimpleSampleInfo(\"Fe3O4\", 1.3) # all fields are present, so no keywords needed\n",
    "\n",
    "simple2 = SimpleSampleInfo(mass = 2.6) # material is missing, so mass must be keyword argument\n",
    "\n",
    "simple3 = SimpleSampleInfo() # since all fields are optional, no arguments are needed\n",
    "simple3.mass = 3.9 # fields can be set after initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get nice `__str__()` and `__repr__()` methods built in to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert str(simple1) == repr(simple1)\n",
    "print(simple1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "\n",
    "Go back to the cell immediately above the one where we defined `Dataclass` and create a `SampleInfo` class that has the following attributes:\n",
    "    - material\n",
    "    - comment\n",
    "    - mass\n",
    "    - volume\n",
    "    - molecular_weight\n",
    "    - size\n",
    "    - shape\n",
    "    - holder\n",
    "    - holder_detail\n",
    "    - offset\n",
    "    - eicosane_mass\n",
    "    - diamagnetic_correction\n",
    "\n",
    "Attritubes are optional with a default value of `None`, and the type of the attribute (if it exists) is either `str` or `float` (e.g. `material` is a `str`, `mass` is a `float`, etc.).\n",
    "\n",
    "Add one method called `RinehartUsage()` to `SampleInfo`. The Rinehart group uses the \"Sample Volume\" and \"Sample Size\" fields in the .dat file to store the eicosane mass and diamagnetic correction, respectively. `RinehartUsage()` function moves those values to their correct fields, sets the \"Sample Volume\" and \"Sample Size\" fields to `None`, and returns `None`.\n",
    "\n",
    "### Exercise 3.8\n",
    "\n",
    "Add a method `_get_sample_info()` to `Dataclass` which reads the header of the first file in `Dataset.files` and returns a `SampleInfo` object with the information from the header. If the header does not include any sample information the function should still return an empty `SampleInfo` object (i.e. all of the attributes are `None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dset1.sample_info.molecular_weight == 1566.22\n",
    "assert dset2_1.sample_info.mass == 0.7\n",
    "assert dset3.sample_info.holder == \"Straw\"\n",
    "assert dset4_1.sample_info.comment == \"brown powder\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Now that we have complete functions and classes, we can add documentation to them. Use the `numpy` formatting standards to add docstrings to `DatFile`, `SampleInfo`, and `Dataset`. Since these are classes, the docstrings should be added to the class definition, not the `__init__()` method. Further, because these classes only contain \"private\" methods (i.e. methods that are only used within the class), we can skip adding docstrings to those classes. Proper type hinting, meangingful variable names, and in-line commenting should be enough for those simple methods to help anyone who needs to use them.\n",
    "\n",
    "From before, here is [a video on code documentation](https://youtu.be/L7Ry-Fiij-M) and here is the [numpy formatting standards](https://numpydoc.readthedocs.io/en/latest/format.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
